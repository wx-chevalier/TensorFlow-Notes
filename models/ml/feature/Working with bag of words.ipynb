{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_name = os.path.join('temp','temp_spam_data.csv')\n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name, 'r') as temp_output_file:\n",
    "        reader = csv.reader(temp_output_file)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    with open(save_file_name, 'w') as temp_output_file:\n",
    "        writer = csv.writer(temp_output_file)\n",
    "        writer.writerows(text_data)\n",
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]\n",
    "target = [1 if x=='spam' else 0 for x in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [x.lower() for x in texts]\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "texts = [' '.join(x.split()) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "sentence_size = 25\n",
    "min_word_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "prediction = tf.sigmoid(model_output)\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Over 4459 Sentences.\n",
      "Training Observation #10: Loss = 0.506898\n",
      "Training Observation #20: Loss = 3.81301\n",
      "Training Observation #30: Loss = 0.355434\n",
      "Training Observation #40: Loss = 0.00821354\n",
      "Training Observation #50: Loss = 0.0805953\n",
      "Training Observation #60: Loss = 0.0576558\n",
      "Training Observation #70: Loss = 0.0153128\n",
      "Training Observation #80: Loss = 0.0127036\n",
      "Training Observation #90: Loss = 2.00682\n",
      "Training Observation #100: Loss = 0.0460043\n",
      "Training Observation #110: Loss = 0.0109576\n",
      "Training Observation #120: Loss = 5.72528\n",
      "Training Observation #130: Loss = 0.00832923\n",
      "Training Observation #140: Loss = 0.00665702\n",
      "Training Observation #150: Loss = 0.0105412\n",
      "Training Observation #160: Loss = 0.00838606\n",
      "Training Observation #170: Loss = 2.15429\n",
      "Training Observation #180: Loss = 3.9443\n",
      "Training Observation #190: Loss = 0.000100375\n",
      "Training Observation #200: Loss = 0.734045\n",
      "Training Observation #210: Loss = 0.056169\n",
      "Training Observation #220: Loss = 2.25785e-06\n",
      "Training Observation #230: Loss = 0.00163657\n",
      "Training Observation #240: Loss = 0.00218928\n",
      "Training Observation #250: Loss = 0.000752992\n",
      "Training Observation #260: Loss = 1.89332\n",
      "Training Observation #270: Loss = 0.000920039\n",
      "Training Observation #280: Loss = 4.75908e-05\n",
      "Training Observation #290: Loss = 2.38466\n",
      "Training Observation #300: Loss = 0.00013803\n",
      "Training Observation #310: Loss = 1.60237\n",
      "Training Observation #320: Loss = 0.00390431\n",
      "Training Observation #330: Loss = 0.000132645\n",
      "Training Observation #340: Loss = 0.0677345\n",
      "Training Observation #350: Loss = 4.67362e-05\n",
      "Training Observation #360: Loss = 3.71228e-05\n",
      "Training Observation #370: Loss = 0.00113547\n",
      "Training Observation #380: Loss = 0.0276257\n",
      "Training Observation #390: Loss = 11.0683\n",
      "Training Observation #400: Loss = 4.98454e-05\n",
      "Training Observation #410: Loss = 3.76644e-05\n",
      "Training Observation #420: Loss = 4.06731e-05\n",
      "Training Observation #430: Loss = 0.00086114\n",
      "Training Observation #440: Loss = 7.7749\n",
      "Training Observation #450: Loss = 10.8904\n",
      "Training Observation #460: Loss = 4.07995\n",
      "Training Observation #470: Loss = 0.00772187\n",
      "Training Observation #480: Loss = 0.00993438\n",
      "Training Observation #490: Loss = 0.00805767\n",
      "Training Observation #500: Loss = 7.24496\n",
      "Training Observation #510: Loss = 0.337381\n",
      "Training Observation #520: Loss = 0.00211923\n",
      "Training Observation #530: Loss = 0.0266998\n",
      "Training Observation #540: Loss = 0.11617\n",
      "Training Observation #550: Loss = 5.86283\n",
      "Training Observation #560: Loss = 3.85416\n",
      "Training Observation #570: Loss = 0.00153654\n",
      "Training Observation #580: Loss = 9.69884e-05\n",
      "Training Observation #590: Loss = 0.0537445\n",
      "Training Observation #600: Loss = 0.0324284\n",
      "Training Observation #610: Loss = 4.41474\n",
      "Training Observation #620: Loss = 2.14823\n",
      "Training Observation #630: Loss = 11.5678\n",
      "Training Observation #640: Loss = 0.00871212\n",
      "Training Observation #650: Loss = 0.158926\n",
      "Training Observation #660: Loss = 2.22225\n",
      "Training Observation #670: Loss = 0.206648\n",
      "Training Observation #680: Loss = 5.76658\n",
      "Training Observation #690: Loss = 0.00482524\n",
      "Training Observation #700: Loss = 0.000388521\n",
      "Training Observation #710: Loss = 0.306463\n",
      "Training Observation #720: Loss = 4.74409e-07\n",
      "Training Observation #730: Loss = 1.97046e-06\n",
      "Training Observation #740: Loss = 0.0830696\n",
      "Training Observation #750: Loss = 0.00439108\n",
      "Training Observation #760: Loss = 3.50972\n",
      "Training Observation #770: Loss = 11.624\n",
      "Training Observation #780: Loss = 2.00573\n",
      "Training Observation #790: Loss = 0.0211393\n",
      "Training Observation #800: Loss = 4.16531\n",
      "Training Observation #810: Loss = 0.000259506\n",
      "Training Observation #820: Loss = 0.000351222\n",
      "Training Observation #830: Loss = 0.000106934\n",
      "Training Observation #840: Loss = 9.8611\n",
      "Training Observation #850: Loss = 2.12464\n",
      "Training Observation #860: Loss = 1.54019e-05\n",
      "Training Observation #870: Loss = 0.00146276\n",
      "Training Observation #880: Loss = 0.00630167\n",
      "Training Observation #890: Loss = 0.110984\n",
      "Training Observation #900: Loss = 0.0204839\n",
      "Training Observation #910: Loss = 1.07937\n",
      "Training Observation #920: Loss = 1.60097e-06\n",
      "Training Observation #930: Loss = 5.01121e-05\n",
      "Training Observation #940: Loss = 9.78011e-05\n",
      "Training Observation #950: Loss = 1.08453e-05\n",
      "Training Observation #960: Loss = 0.0086683\n",
      "Training Observation #970: Loss = 0.0538795\n",
      "Training Observation #980: Loss = 0.0579709\n",
      "Training Observation #990: Loss = 0.000250711\n",
      "Training Observation #1000: Loss = 3.58428\n",
      "Training Observation #1010: Loss = 0.00187523\n",
      "Training Observation #1020: Loss = 0.000561852\n",
      "Training Observation #1030: Loss = 8.34885e-05\n",
      "Training Observation #1040: Loss = 10.8066\n",
      "Training Observation #1050: Loss = 0.000488796\n",
      "Training Observation #1060: Loss = 0.00169738\n",
      "Training Observation #1070: Loss = 0.000117163\n",
      "Training Observation #1080: Loss = 2.65651\n",
      "Training Observation #1090: Loss = 2.04356e-06\n",
      "Training Observation #1100: Loss = 9.32393\n",
      "Training Observation #1110: Loss = 0.000147429\n",
      "Training Observation #1120: Loss = 0.378012\n",
      "Training Observation #1130: Loss = 3.45121e-06\n",
      "Training Observation #1140: Loss = 0.00270174\n",
      "Training Observation #1150: Loss = 0.00210456\n",
      "Training Observation #1160: Loss = 0.00281631\n",
      "Training Observation #1170: Loss = 3.84064\n",
      "Training Observation #1180: Loss = 0.0308198\n",
      "Training Observation #1190: Loss = 0.000883351\n",
      "Training Observation #1200: Loss = 9.06568e-06\n",
      "Training Observation #1210: Loss = 1.03612\n",
      "Training Observation #1220: Loss = 0.0980929\n",
      "Training Observation #1230: Loss = 6.92213e-05\n",
      "Training Observation #1240: Loss = 0.000109142\n",
      "Training Observation #1250: Loss = 5.47426e-05\n",
      "Training Observation #1260: Loss = 3.32884e-05\n",
      "Training Observation #1270: Loss = 9.69559\n",
      "Training Observation #1280: Loss = 1.5989\n",
      "Training Observation #1290: Loss = 2.41058e-06\n",
      "Training Observation #1300: Loss = 8.06285\n",
      "Training Observation #1310: Loss = 2.08394e-05\n",
      "Training Observation #1320: Loss = 0.00318499\n",
      "Training Observation #1330: Loss = 1.26713\n",
      "Training Observation #1340: Loss = 5.87655e-06\n",
      "Training Observation #1350: Loss = 6.06894\n",
      "Training Observation #1360: Loss = 4.53513e-06\n",
      "Training Observation #1370: Loss = 8.68327e-07\n",
      "Training Observation #1380: Loss = 3.95255e-06\n",
      "Training Observation #1390: Loss = 1.46962\n",
      "Training Observation #1400: Loss = 2.05719e-05\n",
      "Training Observation #1410: Loss = 0.167969\n",
      "Training Observation #1420: Loss = 5.48893\n",
      "Training Observation #1430: Loss = 0.000144666\n",
      "Training Observation #1440: Loss = 0.0114444\n",
      "Training Observation #1450: Loss = 0.0212079\n",
      "Training Observation #1460: Loss = 0.00199185\n",
      "Training Observation #1470: Loss = 0.277997\n",
      "Training Observation #1480: Loss = 7.36204e-07\n",
      "Training Observation #1490: Loss = 0.00152064\n",
      "Training Observation #1500: Loss = 7.79812e-05\n",
      "Training Observation #1510: Loss = 0.113708\n",
      "Training Observation #1520: Loss = 7.6556\n",
      "Training Observation #1530: Loss = 0.020329\n",
      "Training Observation #1540: Loss = 4.89822\n",
      "Training Observation #1550: Loss = 0.00183308\n",
      "Training Observation #1560: Loss = 3.13536e-05\n",
      "Training Observation #1570: Loss = 4.39066\n",
      "Training Observation #1580: Loss = 8.35748e-06\n",
      "Training Observation #1590: Loss = 1.23673\n",
      "Training Observation #1600: Loss = 0.00339619\n",
      "Training Observation #1610: Loss = 3.53376e-05\n",
      "Training Observation #1620: Loss = 6.15816e-05\n",
      "Training Observation #1630: Loss = 1.10521e-05\n",
      "Training Observation #1640: Loss = 0.00204654\n",
      "Training Observation #1650: Loss = 0.0761262\n",
      "Training Observation #1660: Loss = 5.26589e-06\n",
      "Training Observation #1670: Loss = 0.26022\n",
      "Training Observation #1680: Loss = 1.07815\n",
      "Training Observation #1690: Loss = 0.0541794\n",
      "Training Observation #1700: Loss = 0.462201\n",
      "Training Observation #1710: Loss = 0.000956642\n",
      "Training Observation #1720: Loss = 6.12601\n",
      "Training Observation #1730: Loss = 0.0108811\n",
      "Training Observation #1740: Loss = 9.41457\n",
      "Training Observation #1750: Loss = 4.59398\n",
      "Training Observation #1760: Loss = 0.00775372\n",
      "Training Observation #1770: Loss = 0.000146911\n",
      "Training Observation #1780: Loss = 4.32783e-06\n",
      "Training Observation #1790: Loss = 0.908951\n",
      "Training Observation #1800: Loss = 0.00794976\n",
      "Training Observation #1810: Loss = 1.47652\n",
      "Training Observation #1820: Loss = 3.75519\n",
      "Training Observation #1830: Loss = 4.12228e-05\n",
      "Training Observation #1840: Loss = 0.000145256\n",
      "Training Observation #1850: Loss = 1.23056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #1860: Loss = 3.44514\n",
      "Training Observation #1870: Loss = 0.869318\n",
      "Training Observation #1880: Loss = 1.62958e-06\n",
      "Training Observation #1890: Loss = 0.220566\n",
      "Training Observation #1900: Loss = 0.0299564\n",
      "Training Observation #1910: Loss = 3.41183\n",
      "Training Observation #1920: Loss = 3.22459e-05\n",
      "Training Observation #1930: Loss = 1.47216e-05\n",
      "Training Observation #1940: Loss = 4.07214e-06\n",
      "Training Observation #1950: Loss = 0.866478\n",
      "Training Observation #1960: Loss = 4.48356e-06\n",
      "Training Observation #1970: Loss = 0.0211716\n",
      "Training Observation #1980: Loss = 0.135092\n",
      "Training Observation #1990: Loss = 0.13763\n",
      "Training Observation #2000: Loss = 9.3294\n",
      "Training Observation #2010: Loss = 8.22131\n",
      "Training Observation #2020: Loss = 0.732771\n",
      "Training Observation #2030: Loss = 1.79193\n",
      "Training Observation #2040: Loss = 0.000407309\n",
      "Training Observation #2050: Loss = 0.00118143\n",
      "Training Observation #2060: Loss = 1.04813e-05\n",
      "Training Observation #2070: Loss = 8.05509e-05\n",
      "Training Observation #2080: Loss = 0.859756\n",
      "Training Observation #2090: Loss = 5.82396e-05\n",
      "Training Observation #2100: Loss = 2.05406e-05\n",
      "Training Observation #2110: Loss = 0.302722\n",
      "Training Observation #2120: Loss = 0.000891324\n",
      "Training Observation #2130: Loss = 3.26438\n",
      "Training Observation #2140: Loss = 0.000403559\n",
      "Training Observation #2150: Loss = 12.7617\n",
      "Training Observation #2160: Loss = 6.32078e-06\n",
      "Training Observation #2170: Loss = 4.491\n",
      "Training Observation #2180: Loss = 4.07293\n",
      "Training Observation #2190: Loss = 0.00958538\n",
      "Training Observation #2200: Loss = 0.0170109\n",
      "Training Observation #2210: Loss = 6.03474e-05\n",
      "Training Observation #2220: Loss = 0.000564875\n",
      "Training Observation #2230: Loss = 0.000385461\n",
      "Training Observation #2240: Loss = 2.44519e-05\n",
      "Training Observation #2250: Loss = 0.000905931\n",
      "Training Observation #2260: Loss = 0.000219324\n",
      "Training Observation #2270: Loss = 1.2757\n",
      "Training Observation #2280: Loss = 0.061866\n",
      "Training Observation #2290: Loss = 11.3099\n",
      "Training Observation #2300: Loss = 0.0144158\n",
      "Training Observation #2310: Loss = 0.379722\n",
      "Training Observation #2320: Loss = 2.21557\n",
      "Training Observation #2330: Loss = 5.55305\n",
      "Training Observation #2340: Loss = 8.58286e-05\n",
      "Training Observation #2350: Loss = 8.49968e-05\n",
      "Training Observation #2360: Loss = 0.243383\n",
      "Training Observation #2370: Loss = 0.000602357\n",
      "Training Observation #2380: Loss = 2.95507\n",
      "Training Observation #2390: Loss = 0.106439\n",
      "Training Observation #2400: Loss = 1.94129\n",
      "Training Observation #2410: Loss = 0.00119933\n",
      "Training Observation #2420: Loss = 0.000155751\n",
      "Training Observation #2430: Loss = 0.000188109\n",
      "Training Observation #2440: Loss = 5.65188e-07\n",
      "Training Observation #2450: Loss = 7.66657\n",
      "Training Observation #2460: Loss = 0.00295414\n",
      "Training Observation #2470: Loss = 1.10359\n",
      "Training Observation #2480: Loss = 1.3659e-05\n",
      "Training Observation #2490: Loss = 0.044292\n",
      "Training Observation #2500: Loss = 1.62792e-05\n",
      "Training Observation #2510: Loss = 0.00400396\n",
      "Training Observation #2520: Loss = 0.000127041\n",
      "Training Observation #2530: Loss = 0.000493169\n",
      "Training Observation #2540: Loss = 2.46063e-06\n",
      "Training Observation #2550: Loss = 0.000238445\n",
      "Training Observation #2560: Loss = 1.11503e-06\n",
      "Training Observation #2570: Loss = 0.00714524\n",
      "Training Observation #2580: Loss = 2.60815e-05\n",
      "Training Observation #2590: Loss = 9.18385e-07\n",
      "Training Observation #2600: Loss = 0.000103351\n",
      "Training Observation #2610: Loss = 0.00473075\n",
      "Training Observation #2620: Loss = 0.25861\n",
      "Training Observation #2630: Loss = 0.254364\n",
      "Training Observation #2640: Loss = 2.03966\n",
      "Training Observation #2650: Loss = 2.94062\n",
      "Training Observation #2660: Loss = 7.3787\n",
      "Training Observation #2670: Loss = 9.38539e-05\n",
      "Training Observation #2680: Loss = 0.00301522\n",
      "Training Observation #2690: Loss = 4.64276e-06\n",
      "Training Observation #2700: Loss = 1.73634e-05\n",
      "Training Observation #2710: Loss = 1.23266e-05\n",
      "Training Observation #2720: Loss = 9.56712e-05\n",
      "Training Observation #2730: Loss = 0.569715\n",
      "Training Observation #2740: Loss = 0.175495\n",
      "Training Observation #2750: Loss = 0.000619857\n",
      "Training Observation #2760: Loss = 2.36183e-06\n",
      "Training Observation #2770: Loss = 8.29077\n",
      "Training Observation #2780: Loss = 0.000174142\n",
      "Training Observation #2790: Loss = 0.532987\n",
      "Training Observation #2800: Loss = 0.148497\n",
      "Training Observation #2810: Loss = 0.000970176\n",
      "Training Observation #2820: Loss = 0.001053\n",
      "Training Observation #2830: Loss = 0.465082\n",
      "Training Observation #2840: Loss = 0.00124871\n",
      "Training Observation #2850: Loss = 4.15124\n",
      "Training Observation #2860: Loss = 1.29264e-05\n",
      "Training Observation #2870: Loss = 4.53426e-06\n",
      "Training Observation #2880: Loss = 3.26338\n",
      "Training Observation #2890: Loss = 9.71152e-06\n",
      "Training Observation #2900: Loss = 9.64091e-06\n",
      "Training Observation #2910: Loss = 1.62518e-06\n",
      "Training Observation #2920: Loss = 1.30803\n",
      "Training Observation #2930: Loss = 0.000276123\n",
      "Training Observation #2940: Loss = 0.00111427\n",
      "Training Observation #2950: Loss = 0.000116835\n",
      "Training Observation #2960: Loss = 0.79289\n",
      "Training Observation #2970: Loss = 4.2186e-06\n",
      "Training Observation #2980: Loss = 5.90084e-05\n",
      "Training Observation #2990: Loss = 0.00744226\n",
      "Training Observation #3000: Loss = 0.000226575\n",
      "Training Observation #3010: Loss = 4.34577e-05\n",
      "Training Observation #3020: Loss = 1.60631\n",
      "Training Observation #3030: Loss = 7.3739\n",
      "Training Observation #3040: Loss = 2.93704\n",
      "Training Observation #3050: Loss = 0.00576915\n",
      "Training Observation #3060: Loss = 0.000264326\n",
      "Training Observation #3070: Loss = 4.76139\n",
      "Training Observation #3080: Loss = 0.0017972\n",
      "Training Observation #3090: Loss = 2.60064e-05\n",
      "Training Observation #3100: Loss = 9.86713\n",
      "Training Observation #3110: Loss = 8.72144e-06\n",
      "Training Observation #3120: Loss = 9.92574e-05\n",
      "Training Observation #3130: Loss = 0.00995051\n",
      "Training Observation #3140: Loss = 7.85925e-05\n",
      "Training Observation #3150: Loss = 0.0139945\n",
      "Training Observation #3160: Loss = 3.62574e-05\n",
      "Training Observation #3170: Loss = 4.97933\n",
      "Training Observation #3180: Loss = 9.05469e-05\n",
      "Training Observation #3190: Loss = 0.0653098\n",
      "Training Observation #3200: Loss = 1.01837e-05\n",
      "Training Observation #3210: Loss = 0.026956\n",
      "Training Observation #3220: Loss = 4.70917\n",
      "Training Observation #3230: Loss = 0.000362586\n",
      "Training Observation #3240: Loss = 0.0334028\n",
      "Training Observation #3250: Loss = 7.4104\n",
      "Training Observation #3260: Loss = 3.99521e-06\n",
      "Training Observation #3270: Loss = 0.0469858\n",
      "Training Observation #3280: Loss = 3.09994e-05\n",
      "Training Observation #3290: Loss = 0.0016591\n",
      "Training Observation #3300: Loss = 3.21614\n",
      "Training Observation #3310: Loss = 0.0615026\n",
      "Training Observation #3320: Loss = 0.711315\n",
      "Training Observation #3330: Loss = 2.01921\n",
      "Training Observation #3340: Loss = 0.000286806\n",
      "Training Observation #3350: Loss = 0.0355728\n",
      "Training Observation #3360: Loss = 0.254368\n",
      "Training Observation #3370: Loss = 0.000626073\n",
      "Training Observation #3380: Loss = 0.000366764\n",
      "Training Observation #3390: Loss = 0.00192633\n",
      "Training Observation #3400: Loss = 0.00179254\n",
      "Training Observation #3410: Loss = 0.0849896\n",
      "Training Observation #3420: Loss = 0.000373164\n",
      "Training Observation #3430: Loss = 0.000101439\n",
      "Training Observation #3440: Loss = 3.82811\n",
      "Training Observation #3450: Loss = 0.000564478\n",
      "Training Observation #3460: Loss = 0.00238024\n",
      "Training Observation #3470: Loss = 9.07954e-05\n",
      "Training Observation #3480: Loss = 0.00171885\n",
      "Training Observation #3490: Loss = 0.282684\n",
      "Training Observation #3500: Loss = 0.000621672\n",
      "Training Observation #3510: Loss = 0.0407183\n",
      "Training Observation #3520: Loss = 0.0131079\n",
      "Training Observation #3530: Loss = 0.000249602\n",
      "Training Observation #3540: Loss = 0.00112753\n",
      "Training Observation #3550: Loss = 0.0002886\n",
      "Training Observation #3560: Loss = 0.053904\n",
      "Training Observation #3570: Loss = 0.0018293\n",
      "Training Observation #3580: Loss = 8.49585e-06\n",
      "Training Observation #3590: Loss = 0.0499763\n",
      "Training Observation #3600: Loss = 4.01768\n",
      "Training Observation #3610: Loss = 0.861092\n",
      "Training Observation #3620: Loss = 0.0095065\n",
      "Training Observation #3630: Loss = 4.83281\n",
      "Training Observation #3640: Loss = 0.846988\n",
      "Training Observation #3650: Loss = 0.117437\n",
      "Training Observation #3660: Loss = 9.39011e-05\n",
      "Training Observation #3670: Loss = 0.0638357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #3680: Loss = 0.00143415\n",
      "Training Observation #3690: Loss = 0.00622355\n",
      "Training Observation #3700: Loss = 0.000849533\n",
      "Training Observation #3710: Loss = 5.7147e-05\n",
      "Training Observation #3720: Loss = 9.47656e-05\n",
      "Training Observation #3730: Loss = 0.00152313\n",
      "Training Observation #3740: Loss = 4.00341e-05\n",
      "Training Observation #3750: Loss = 0.0261611\n",
      "Training Observation #3760: Loss = 0.00141383\n",
      "Training Observation #3770: Loss = 0.00619028\n",
      "Training Observation #3780: Loss = 0.183325\n",
      "Training Observation #3790: Loss = 0.428427\n",
      "Training Observation #3800: Loss = 0.436703\n",
      "Training Observation #3810: Loss = 6.765\n",
      "Training Observation #3820: Loss = 0.00150894\n",
      "Training Observation #3830: Loss = 3.91251e-05\n",
      "Training Observation #3840: Loss = 0.0193604\n",
      "Training Observation #3850: Loss = 0.000298647\n",
      "Training Observation #3860: Loss = 0.000137359\n",
      "Training Observation #3870: Loss = 0.00612465\n",
      "Training Observation #3880: Loss = 0.000402474\n",
      "Training Observation #3890: Loss = 0.0595582\n",
      "Training Observation #3900: Loss = 0.000499797\n",
      "Training Observation #3910: Loss = 2.36445e-05\n",
      "Training Observation #3920: Loss = 2.14935e-05\n",
      "Training Observation #3930: Loss = 4.6998e-05\n",
      "Training Observation #3940: Loss = 4.3821\n",
      "Training Observation #3950: Loss = 0.00576125\n",
      "Training Observation #3960: Loss = 8.32278e-06\n",
      "Training Observation #3970: Loss = 0.0571901\n",
      "Training Observation #3980: Loss = 0.0369103\n",
      "Training Observation #3990: Loss = 0.00312247\n",
      "Training Observation #4000: Loss = 1.75517e-05\n",
      "Training Observation #4010: Loss = 0.00270936\n",
      "Training Observation #4020: Loss = 0.0353634\n",
      "Training Observation #4030: Loss = 0.00282806\n",
      "Training Observation #4040: Loss = 0.000183248\n",
      "Training Observation #4050: Loss = 2.89758e-05\n",
      "Training Observation #4060: Loss = 1.17775\n",
      "Training Observation #4070: Loss = 0.000193583\n",
      "Training Observation #4080: Loss = 0.000285679\n",
      "Training Observation #4090: Loss = 0.00663443\n",
      "Training Observation #4100: Loss = 0.0109183\n",
      "Training Observation #4110: Loss = 0.00207786\n",
      "Training Observation #4120: Loss = 0.0102123\n",
      "Training Observation #4130: Loss = 0.000214398\n",
      "Training Observation #4140: Loss = 0.217228\n",
      "Training Observation #4150: Loss = 1.63538e-06\n",
      "Training Observation #4160: Loss = 0.00156639\n",
      "Training Observation #4170: Loss = 3.90177\n",
      "Training Observation #4180: Loss = 0.0550672\n",
      "Training Observation #4190: Loss = 3.20254e-06\n",
      "Training Observation #4200: Loss = 0.000156461\n",
      "Training Observation #4210: Loss = 0.068662\n",
      "Training Observation #4220: Loss = 4.52486\n",
      "Training Observation #4230: Loss = 5.28878e-06\n",
      "Training Observation #4240: Loss = 0.00929232\n",
      "Training Observation #4250: Loss = 7.70285e-06\n",
      "Training Observation #4260: Loss = 0.000398633\n",
      "Training Observation #4270: Loss = 0.000199898\n",
      "Training Observation #4280: Loss = 0.0205489\n",
      "Training Observation #4290: Loss = 0.00547727\n",
      "Training Observation #4300: Loss = 1.21237\n",
      "Training Observation #4310: Loss = 7.1575e-06\n",
      "Training Observation #4320: Loss = 0.0936356\n",
      "Training Observation #4330: Loss = 1.85319\n",
      "Training Observation #4340: Loss = 2.92462\n",
      "Training Observation #4350: Loss = 0.000191486\n",
      "Training Observation #4360: Loss = 0.00295229\n",
      "Training Observation #4370: Loss = 0.00211146\n",
      "Training Observation #4380: Loss = 0.000776866\n",
      "Training Observation #4390: Loss = 2.31089\n",
      "Training Observation #4400: Loss = 9.63873e-05\n",
      "Training Observation #4410: Loss = 0.000156667\n",
      "Training Observation #4420: Loss = 0.000157158\n",
      "Training Observation #4430: Loss = 0.0793177\n",
      "Training Observation #4440: Loss = 2.52518\n",
      "Training Observation #4450: Loss = 0.0399658\n"
     ]
    }
   ],
   "source": [
    "print('Starting Training Over {} Sentences.'.format(len(texts_train)))\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    if (ix+1)%10==0:\n",
    "        print('Training Observation #' + str(ix+1) + ': Loss = ' + str(temp_loss))\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    if len(train_acc_all) >= 50:\n",
    "        train_acc_avg.append(np.mean(train_acc_all[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Test Set Accuracy For 1115 Sentences.\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "\n",
      "Overall Test Accuracy: 0.8152466367713005\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe5edd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\n",
    "test_acc_all = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    if (ix+1)%50==0:\n",
    "        print('Test Observation #' + str(ix+1))\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    test_acc_temp = target_test[ix]==np.round(temp_pred)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "print('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))\n",
    "plt.plot(range(len(train_acc_avg)), train_acc_avg, 'k-', label='Train Accuracy')\n",
    "plt.title('Avg Training Acc Over Past 50 Generations')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
