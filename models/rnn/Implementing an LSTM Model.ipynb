{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "min_word_freq = 5\n",
    "rnn_size = 128\n",
    "embedding_size = 100\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_seq_len = 50\n",
    "embedding_size = rnn_size\n",
    "save_every = 500\n",
    "eval_every = 50\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n",
      "Not found, downloading Shakespeare texts from www.gutenberg.org\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "print('Loading Shakespeare Data')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    s_text = s_text[7675:]\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length = 8009\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    ix_to_vocab_dict = {val:key for key,val in vocab_to_ix_dict.items()}\n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "assert(len(ix2vocab) == len(vocab2ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "\n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "\n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        def inferred_loop(prev, count):\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.73\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 9.16\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.70\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.32\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 8.23\n",
      "thou art more suit trouble satisfy utt'red trouble opposed unarm'd unarm'd alms to\n",
      "to be or not to the\n",
      "wherefore art thou respects grave wilt cooling strato panthino grief and\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.91\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.69\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.49\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.37\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.09\n",
      "thou art more staves show 1 for your\n",
      "to be or not to the\n",
      "wherefore art thou art protest hood hood us and\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 6.88\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.97\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.68\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.74\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.71\n",
      "thou art more than than it is not to the\n",
      "to be or not to be a\n",
      "wherefore art thou art protest hood makes a\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.77\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.83\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.78\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 7.19\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 7.24\n",
      "thou art more than than it is not to the\n",
      "to be or not to the\n",
      "wherefore art thou art hast thou art thou art thou art thou art\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.71\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.33\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.23\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.33\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.45\n",
      "thou art more than than our\n",
      "to be or not to the\n",
      "wherefore art thou art not speak it be\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.30\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.23\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.30\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.46\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.33\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art not speak it be a\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.13\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.15\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.29\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.09\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.35\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art men's her her her her her her her her\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.32\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.36\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.01\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.01\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.26\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art there's not a\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.15\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 5.94\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.18\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.15\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.04\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art not speak it not a\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.32\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.02\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.42\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.18\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.20\n",
      "Model Saved To: temp\\shakespeare_model\\model\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art there's a\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.13\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.28\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.20\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.21\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.05\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 6.20\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 5.93\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.00\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 5.94\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.04\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou hast thy thy thy patience thy\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.05\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.12\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 5.83\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.00\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.03\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 6.26\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.08\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 5.98\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.32\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 5.98\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.08\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 5.95\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 6.07\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 6.12\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.16\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thy\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 5.83\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 6.09\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 6.24\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 6.11\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 6.03\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 6.06\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 6.01\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.05\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 6.01\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 6.17\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art not not a\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 6.22\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 6.05\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 6.01\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 6.08\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 5.94\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou hast thou shalt not be a\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 5.90\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 5.85\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.88\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.91\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 5.94\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 6.12\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 6.17\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.77\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 6.07\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.95\n",
      "Model Saved To: temp\\shakespeare_model\\model\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 6.27\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 5.88\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 6.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 5.63\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 5.88\n",
      "thou art more than our\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 5.90\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.90\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 6.00\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.93\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 6.11\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.81\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.84\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 6.15\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 6.17\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.85\n",
      "thou art more than that\n",
      "to be or not to the\n",
      "wherefore art thou hast thy\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 5.60\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 6.03\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.83\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.88\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.80\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou know'st not in the\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 5.82\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 6.08\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 6.16\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.83\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 5.88\n",
      "thou art more than that\n",
      "to be or not to be\n",
      "wherefore art thou wilt be\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 5.88\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.79\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.75\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 6.02\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.80\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.81\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.89\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 6.14\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 5.99\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.86\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou shalt not be a\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.74\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 6.13\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.97\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.83\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.86\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou know'st not as i am a\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 6.00\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.83\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 6.03\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.74\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.75\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.78\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.76\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.86\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.81\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.90\n",
      "Model Saved To: temp\\shakespeare_model\\model\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou not not a\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.92\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.88\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.87\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.79\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.82\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou not not a\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 5.95\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 5.81\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.87\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.61\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.81\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou know'st not a\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.79\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.58\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.88\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.79\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.65\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.72\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.41\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.92\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.82\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.76\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou not not a\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.72\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.93\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 5.63\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.89\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 6.04\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou not be\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.78\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.70\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 5.81\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.57\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.66\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou not not a\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.84\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(batches)\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        if iteration_count % save_every == 0:\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ4nAGGMynCUCY4zJcP8fd9SUdmk/fxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160716a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
